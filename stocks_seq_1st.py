# -*- coding: utf-8 -*-
"""Stocks_seq_1st.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/sfwtopoulos/stocks/blob/master/Stocks_seq_1st.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
import math
import warnings
import numpy as np
import time

import pandas_datareader as pdr
#%tensorflow_version 2.x
# Restart runtime using 'Runtime' -> 'Restart runtime...'
# %tensorflow_version 1.x
import tensorflow as tf
import keras


from keras.layers import LSTM
from keras.models import Sequential
from keras.layers.wrappers import TimeDistributed
from keras.layers.core import Dense, Activation, Dropout
from keras.preprocessing.sequence import pad_sequences

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt
#not all needed


warnings.filterwarnings('ignore',category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

print(keras.__version__)
print(tf.__version__)
#print(tensorflow.compat.v2.__version__)

#Data import from git repo
url = 'https://raw.githubusercontent.com/sfwtopoulos/stocks/master/stocks_dataset/combined.csv'
#df1 = pd.read_csv(url, error_bad_lines=False)
dfstocks = pd.read_csv(url, sep=',')

#fix column names
dfstocks=dfstocks.rename({' AAPL':'Company', ' Close/Last':'Close', ' Volume':'Volume', ' Open':'Open', ' High':'High', ' Low':'Low'}, axis=1);
for col in dfstocks.columns: 
    print(col)

#drop $sign from values
dfstocks.Close=dfstocks['Close'].astype(str)
dfstocks.Close=dfstocks.Close.apply(lambda x: x.replace('$',''))
dfstocks.Open=dfstocks['Open'].astype(str)
dfstocks.Open=dfstocks.Open.apply(lambda x: x.replace('$',''))
dfstocks.High=dfstocks['High'].astype(str)
dfstocks.High=dfstocks.High.apply(lambda x: x.replace('$',''))
dfstocks.Low=dfstocks['Low'].astype(str)
dfstocks.Low=dfstocks.Low.apply(lambda x: x.replace('$',''))

#drop rows containing nan or header from the csv files
dfstocks=dfstocks[~dfstocks.Low.str.contains("nan")]
dfstocks=dfstocks[~dfstocks.Low.str.contains("Low")]
dfstocks=dfstocks[~dfstocks.Low.str.contains("N/A")]
dfstocks=dfstocks[~dfstocks.Volume.str.contains("N/A")]
dfstocks.reset_index(drop=True, inplace=True)
dfstocks.Date.count()

#sort dataframe based on date and Company Name
dfstocks = dfstocks.sort_values(['Date', 'Company'])
dfstocks.head(100)

#Test Split
#dfstocks_split = dfstocks.sample(frac=0.9998,random_state=200)
#sub_split=dfstocks.drop(dfstocks_split.index)
# sub_split = sub_split.reindex(index=sub_split.Date(sub_split.index.min(), 
#                                           sub_split.index.max(), 
#                                           freq='D')).fillna(method='ffill')

#Converting Datatypes
dfstocks.Date=pd.to_datetime(dfstocks.Date)
dfstocks.Close=pd.to_numeric(dfstocks.Close)
dfstocks.Volume=pd.to_numeric(dfstocks.Volume)
dfstocks.Open=pd.to_numeric(dfstocks.Open)
dfstocks.High=pd.to_numeric(dfstocks.High)
dfstocks.Low=pd.to_numeric(dfstocks.Low)
dfstocks.dtypes

#dfstocks[dfstocks['Company'].str.contains("AMZN")]
sub_split=dfstocks[dfstocks['Company'].str.contains("AAPL")]
sub_split.isnull().values.any()
sub_split.isna().values.any()
sub_split = sub_split.sort_values(['Date'])
sub_split.reset_index(drop=True, inplace=True)
sub_split.shape[0]

sub_split

TRAIN_PERCENT = 0.7
#STOCK_INDEX = '^GSPC'
VERBOSE=True

# prepare training and testing data sets for LSTM based sequence modeling
#def get_seq_train_test(time_series, scaling=True,train_size=0.9):
def get_seq_train_test(time_series,train_size=0.9):
    # scaler = None
    # if scaling:
    #scaling of variables to range [0,1]
    #must scale seperatly or scale only the output variable maybe!??
    scaler = MinMaxScaler(feature_range=(0, 1))
    #reshape(-1,1) -1 --> unknown number of rows, 1 --> 1 column
    #time_series = np.array(time_series).reshape(-1,1)
    time_series = np.array(time_series).reshape(-1,2)
    #time_series = np.array(time_series)
    scaled_stock_series = scaler.fit_transform(time_series)
    # else:
    #     scaled_stock_series = time_series
    #scaled_stock_series = time_series
    time_series = scaled_stock_series
        
    #train_size = int(len(scaled_stock_series) * train_size)
    train_size = int(len(time_series) * train_size)

    #train = scaled_stock_series[0:train_size]
    train = time_series[0:train_size]
    #test = scaled_stock_series[train_size:len(scaled_stock_series)]
    test = time_series[train_size:len(time_series)]
    
    return train,test,scaler

def get_seq_model(hidden_units=4,input_shape=(1,1),verbose=False):
    # create and fit the LSTM network
    model = Sequential()
    # samples*timesteps*featuress
    # https://keras.io/getting-started/sequential-model-guide/
    #relu_advanced=keras.activations.relu(x,)

    model.add(LSTM(input_shape=input_shape,
                   units = hidden_units,  
   #                activation='relu',
                   return_sequences=True
    ))

    # readout layer. TimeDistributedDense uses the same weights for all
    # time steps.
    # model.add(TimeDistributed(Dense(1))) #number of inputs in the 1st layer
    model.add(TimeDistributed(Dense(2)))
   
    start = time.time()
    #softsign
    #model.add(activation='softsign')


    model.compile(loss="mse", optimizer="rmsprop")

    if verbose:
        print("> Compilation Time : ", time.time() - start)
        print(model.summary())

    return model

type(sub_split.Close)
sub_split=sub_split[['Volume', 'Close']]

# split train and test datasets
train,test,scaler = get_seq_train_test(sub_split,
#                                  scaling=True,
                                   train_size=TRAIN_PERCENT)

#train
print(len(train))
print(train.shape[0])
print(train.shape)
print(test.shape)

#We use numpy to reshape our time series into 3D tensors.
train = np.reshape(train,(1,train.shape[0],2))
test = np.reshape(test,(1,test.shape[0],2))

train_x = train[:,:-1,:]
train_y = train[:,1:,:]

test_x = test[:,:-1,:]
test_y = test[:,1:,:]

print("Data Split Complete")

print("train_x shape={}".format(train_x.shape))
print("train_y shape={}".format(train_y.shape))
print("test_x shape={}".format(test_x.shape))
print("test_y shape={}".format(test_y.shape))

# build RNN model
seq_lstm_model=None
try:
    seq_lstm_model = get_seq_model(input_shape=(train_x.shape[1],2),
                                                verbose=VERBOSE)
except:
    print("Model Build Failed. Trying Again")
    seq_lstm_model = get_seq_model(input_shape=(train_x.shape[1],2),
                                                verbose=VERBOSE)

# train the model
seq_lstm_model.fit(train_x, train_y,
                #epochs=250, batch_size=1,
                epochs=250, batch_size=1,
                verbose=2)
print("Model Fit Complete")

# train fit performance
trainPredict = seq_lstm_model.predict(train_x)
trainScore = math.sqrt(mean_squared_error(train_y[0], trainPredict[0]))
print('Train Score: %.2f RMSE' % (trainScore))

trainPredict

# Pad input sequence
testPredict = pad_sequences(test_x,maxlen=train_x.shape[1],padding='post',dtype='float64')

# forecast values
testPredict = seq_lstm_model.predict(testPredict)
testPredict

# forecast values
testPredict = seq_lstm_model.predict(testPredict)
testPredict

testPredict

scaler

# # inverse transformation

#scaler = MinMaxScaler(feature_range=(0, 1))
#time_series = np.array(sub_split.Close).reshape(-1,1)
#scaler = scaler.fit(time_series)

#trainPredict=trainPredict.reshape(trainPredict.shape[1])
#testPredict=testPredict.reshape(testPredict.shape[1])


 #scaler = MinMaxScaler(feature_range=(0, 1))

# trainPredict = scaler.inverse_transform(trainPredict.reshape(trainPredict.shape[1]))
# testPredict = scaler.inverse_transform(testPredict.reshape(testPredict.shape[1]))

trainPredict = scaler.inverse_transform(trainPredict.\
                                        reshape(-1,2))
testPredict = scaler.inverse_transform(testPredict.\
                                       reshape(-1,2))

# # inverse transformation
# trainPredict = scaler.inverse_transform(trainPredict.\
#                                         reshape(trainPredict.shape[1],2))
# testPredict = scaler.inverse_transform(testPredict.\
#                                         reshape(testPredict.shape[1],2))

trainPredict

sub_split_date=dfstocks[dfstocks['Company'].str.contains("AAPL")]
sub_split_date.isnull().values.any()
sub_split_date.isna().values.any()
sub_split_date = sub_split_date.sort_values(['Date'])
sub_split_date.reset_index(drop=True, inplace=True)
sub_split_date.shape[0]

# plot the true and forecasted values
train_size = len(trainPredict)+1
#sub_split.Date.dt.year
# plt.plot(sub_split.index,
#           sub_split.Close.values,c='black',
#           alpha=0.3,label='True Data')
plt.plot(sub_split_date.Date,
          sub_split.Close.values,c='black',
          alpha=0.3,label='True Data')
# # multipl x2
# plt.plot(sub_split_date.Date[1:train_size],
#           trainPredict,label='Training Fit',c='g')
# #mulitple x3
# plt.plot(sub_split_date.Date[train_size+1:],
#           testPredict[:test_x.shape[1]],label='Testing Forecast')
plt.title('Forecast Plot')
plt.legend()
plt.show()
plt.rcParams["figure.figsize"] = (20,10)
plt.show()