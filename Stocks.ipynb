{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfwtopoulos/stocks/blob/master/Stocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qemdNp8GaAQL",
        "colab_type": "code",
        "outputId": "5d605e20-887d-4ec5-b9c9-33c67fd83952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!git init\n",
        "#!git config --global user.email \"sfwtopoulos@gmail.com\"\n",
        "#!git config --global user.name \"sfwtopoulos\"\n",
        "#!git add -A\n",
        "#!git commit -m \"YO\"\n",
        "#!git remote rm origin\n",
        "#!git remote add origin https://sfwtopoulos:%21%3D%25%28wy%27%22E3%2BE4WJK8%3C%25Sgithub@github.com/sfwtopoulos/stocks.git\n",
        "#!git clone https://github.com/sfwtopoulos/stocks\n",
        "#!git pull\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiMnCcpheA2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas_datareader as pdr\n",
        "\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# specify to ignore warning messages\n",
        "#warnings.filterwarnings(\"ignore\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCCmauyULvM6",
        "colab_type": "code",
        "outputId": "00035ee8-59cc-4801-bef3-94a1e4c21cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.layers.core import Dense, Activation, Dropout"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0IZA2fPUD-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#not all needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1GtvT7RSUkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data import from git repo\n",
        "url = 'https://raw.githubusercontent.com/sfwtopoulos/stocks/master/stocks_dataset/combined.csv'\n",
        "#df1 = pd.read_csv(url, error_bad_lines=False)\n",
        "dfstocks = pd.read_csv(url, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXbxTv7dN3Qy",
        "colab_type": "code",
        "outputId": "f96e0d36-e624-432e-bcdc-6f7d95e7a1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "for col in dfstocks.columns: \n",
        "    print(col) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Date\n",
            " Close/Last\n",
            " Volume\n",
            " Open\n",
            " High\n",
            " Low\n",
            " AAPL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brASTrbbCiQ7",
        "colab_type": "code",
        "outputId": "3284811e-e162-464b-cd14-9380754f2f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#fix column names\n",
        "dfstocks=dfstocks.rename({' AAPL':'Company', ' Close/Last':'Close', ' Volume':'Volume', ' Open':'Open', ' High':'High', ' Low':'Low'}, axis=1);\n",
        "for col in dfstocks.columns: \n",
        "    print(col) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Date\n",
            "Close\n",
            "Volume\n",
            "Open\n",
            "High\n",
            "Low\n",
            "Company\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a22eQ-a9PxVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop $sign from values\n",
        "dfstocks.Close=dfstocks['Close'].astype(str)\n",
        "dfstocks.Close=dfstocks.Close.apply(lambda x: x.replace('$',''))\n",
        "dfstocks.Open=dfstocks['Open'].astype(str)\n",
        "dfstocks.Open=dfstocks.Open.apply(lambda x: x.replace('$',''))\n",
        "dfstocks.High=dfstocks['High'].astype(str)\n",
        "dfstocks.High=dfstocks.High.apply(lambda x: x.replace('$',''))\n",
        "dfstocks.Low=dfstocks['Low'].astype(str)\n",
        "dfstocks.Low=dfstocks.Low.apply(lambda x: x.replace('$',''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afYv0Wbycmb6",
        "colab_type": "code",
        "outputId": "48b88b4d-305a-4073-b548-7e24557854db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#drop rows containing nan or header from the csv files\n",
        "dfstocks=dfstocks[~dfstocks.Low.str.contains(\"nan\")]\n",
        "dfstocks=dfstocks[~dfstocks.Low.str.contains(\"Low\")]\n",
        "dfstocks.reset_index(drop=True, inplace=True)\n",
        "dfstocks.Date.count()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "229286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DG2XfzYa649",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sort dataframe based on date and Company Name\n",
        "  dfstocks = dfstocks.sort_values(['Date', 'Company'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCqTF2_mPxpN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRIDlF4HdO2",
        "colab_type": "code",
        "outputId": "a92a55e2-d026-468f-d45a-c162d351c1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "dfstocks.head(100)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Company</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1761</th>\n",
              "      <td>01/02/2013</td>\n",
              "      <td>78.4328</td>\n",
              "      <td>139906732</td>\n",
              "      <td>79.1171</td>\n",
              "      <td>79.2857</td>\n",
              "      <td>77.3757</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4248</th>\n",
              "      <td>01/02/2013</td>\n",
              "      <td>35.12</td>\n",
              "      <td>13767660</td>\n",
              "      <td>34.92</td>\n",
              "      <td>35.4</td>\n",
              "      <td>34.1</td>\n",
              "      <td>ABBV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6025</th>\n",
              "      <td>01/02/2013</td>\n",
              "      <td>32.05</td>\n",
              "      <td>20266410</td>\n",
              "      <td>32.3</td>\n",
              "      <td>32.45</td>\n",
              "      <td>31.64</td>\n",
              "      <td>ABT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8512</th>\n",
              "      <td>01/02/2013</td>\n",
              "      <td>69.06</td>\n",
              "      <td>4039095</td>\n",
              "      <td>67.59</td>\n",
              "      <td>69.06</td>\n",
              "      <td>67.55</td>\n",
              "      <td>ACN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10999</th>\n",
              "      <td>01/02/2013</td>\n",
              "      <td>38.34</td>\n",
              "      <td>6483720</td>\n",
              "      <td>37.92</td>\n",
              "      <td>38.73</td>\n",
              "      <td>37.92</td>\n",
              "      <td>ADBE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10747</th>\n",
              "      <td>01/02/2014</td>\n",
              "      <td>59.29</td>\n",
              "      <td>2745895</td>\n",
              "      <td>59.06</td>\n",
              "      <td>59.53</td>\n",
              "      <td>58.94</td>\n",
              "      <td>ADBE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13234</th>\n",
              "      <td>01/02/2014</td>\n",
              "      <td>168.05</td>\n",
              "      <td>1268722</td>\n",
              "      <td>167.33</td>\n",
              "      <td>170.71</td>\n",
              "      <td>166.5937</td>\n",
              "      <td>AGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15721</th>\n",
              "      <td>01/02/2014</td>\n",
              "      <td>50.71</td>\n",
              "      <td>9196092</td>\n",
              "      <td>50.81</td>\n",
              "      <td>51.3</td>\n",
              "      <td>50.47</td>\n",
              "      <td>AIG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18208</th>\n",
              "      <td>01/02/2014</td>\n",
              "      <td>53.55</td>\n",
              "      <td>1909106</td>\n",
              "      <td>54.09</td>\n",
              "      <td>54.45</td>\n",
              "      <td>53.5</td>\n",
              "      <td>ALL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20695</th>\n",
              "      <td>01/02/2014</td>\n",
              "      <td>115.795</td>\n",
              "      <td>2528751</td>\n",
              "      <td>114.36</td>\n",
              "      <td>116.19</td>\n",
              "      <td>114.06</td>\n",
              "      <td>AMGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Date     Close      Volume      Open      High        Low Company\n",
              "1761   01/02/2013   78.4328   139906732   79.1171   79.2857    77.3757    AAPL\n",
              "4248   01/02/2013     35.12    13767660     34.92      35.4       34.1    ABBV\n",
              "6025   01/02/2013     32.05    20266410      32.3     32.45      31.64     ABT\n",
              "8512   01/02/2013     69.06     4039095     67.59     69.06      67.55     ACN\n",
              "10999  01/02/2013     38.34     6483720     37.92     38.73      37.92    ADBE\n",
              "...           ...       ...         ...       ...       ...        ...     ...\n",
              "10747  01/02/2014     59.29     2745895     59.06     59.53      58.94    ADBE\n",
              "13234  01/02/2014    168.05     1268722    167.33    170.71   166.5937     AGN\n",
              "15721  01/02/2014     50.71     9196092     50.81      51.3      50.47     AIG\n",
              "18208  01/02/2014     53.55     1909106     54.09     54.45       53.5     ALL\n",
              "20695  01/02/2014   115.795     2528751    114.36    116.19     114.06    AMGN\n",
              "\n",
              "[100 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcuzyarDrw5l",
        "colab_type": "code",
        "outputId": "1587e341-6a49-493c-a4f8-e33a8dcb3dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dfstocks_split = dfstocks.sample(frac=0.9998,random_state=200)\n",
        "second_split=dfstocks.drop(dfstocks_split.index)\n",
        "second_split"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Company</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>159523</th>\n",
              "      <td>01/04/2016</td>\n",
              "      <td>103.57</td>\n",
              "      <td>1929089</td>\n",
              "      <td>102.87</td>\n",
              "      <td>103.65</td>\n",
              "      <td>102.34</td>\n",
              "      <td>NEE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173214</th>\n",
              "      <td>01/05/2011</td>\n",
              "      <td>94.1728</td>\n",
              "      <td>3597597</td>\n",
              "      <td>93.3963</td>\n",
              "      <td>94.7261</td>\n",
              "      <td>92.5616</td>\n",
              "      <td>OXY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15212</th>\n",
              "      <td>01/11/2016</td>\n",
              "      <td>57.69</td>\n",
              "      <td>9615965</td>\n",
              "      <td>57.89</td>\n",
              "      <td>58</td>\n",
              "      <td>56.84</td>\n",
              "      <td>AIG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95258</th>\n",
              "      <td>01/15/2015</td>\n",
              "      <td>23.5037</td>\n",
              "      <td>31431330</td>\n",
              "      <td>23.7828</td>\n",
              "      <td>23.8925</td>\n",
              "      <td>23.4838</td>\n",
              "      <td>GE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219831</th>\n",
              "      <td>01/16/2018</td>\n",
              "      <td>133.97</td>\n",
              "      <td>5170230</td>\n",
              "      <td>137.73</td>\n",
              "      <td>137.73</td>\n",
              "      <td>133.87</td>\n",
              "      <td>UTX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38340</th>\n",
              "      <td>01/25/2013</td>\n",
              "      <td>718.82</td>\n",
              "      <td>1570859</td>\n",
              "      <td>682</td>\n",
              "      <td>719.37</td>\n",
              "      <td>681.05</td>\n",
              "      <td>BKNG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183867</th>\n",
              "      <td>02/05/2018</td>\n",
              "      <td>74.705</td>\n",
              "      <td>18709640</td>\n",
              "      <td>75.95</td>\n",
              "      <td>78.98</td>\n",
              "      <td>74.68</td>\n",
              "      <td>PYPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211127</th>\n",
              "      <td>02/06/2013</td>\n",
              "      <td>57.38</td>\n",
              "      <td>5266009</td>\n",
              "      <td>57.44</td>\n",
              "      <td>57.54</td>\n",
              "      <td>57.1</td>\n",
              "      <td>UNH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214108</th>\n",
              "      <td>02/17/2011</td>\n",
              "      <td>48.91</td>\n",
              "      <td>6399682</td>\n",
              "      <td>48.77</td>\n",
              "      <td>48.98</td>\n",
              "      <td>48.165</td>\n",
              "      <td>UNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166448</th>\n",
              "      <td>02/20/2018</td>\n",
              "      <td>249.08</td>\n",
              "      <td>17117670</td>\n",
              "      <td>244.75</td>\n",
              "      <td>251.87</td>\n",
              "      <td>244.6</td>\n",
              "      <td>NVDA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124920</th>\n",
              "      <td>02/26/2015</td>\n",
              "      <td>42.46</td>\n",
              "      <td>14344290</td>\n",
              "      <td>42.04</td>\n",
              "      <td>42.55</td>\n",
              "      <td>41.97</td>\n",
              "      <td>KO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9693</th>\n",
              "      <td>03/12/2018</td>\n",
              "      <td>220.94</td>\n",
              "      <td>3172266</td>\n",
              "      <td>222.97</td>\n",
              "      <td>223.11</td>\n",
              "      <td>220.34</td>\n",
              "      <td>ADBE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>03/23/2015</td>\n",
              "      <td>60.48</td>\n",
              "      <td>7918489</td>\n",
              "      <td>60.05</td>\n",
              "      <td>61.19</td>\n",
              "      <td>59.62</td>\n",
              "      <td>ABBV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68129</th>\n",
              "      <td>04/15/2013</td>\n",
              "      <td>56.31</td>\n",
              "      <td>6086714</td>\n",
              "      <td>57.33</td>\n",
              "      <td>57.53</td>\n",
              "      <td>56.3</td>\n",
              "      <td>CVS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16396</th>\n",
              "      <td>04/27/2011</td>\n",
              "      <td>31.7</td>\n",
              "      <td>5420339</td>\n",
              "      <td>32.01</td>\n",
              "      <td>32.01</td>\n",
              "      <td>31.28</td>\n",
              "      <td>AIG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79605</th>\n",
              "      <td>04/30/2010</td>\n",
              "      <td>50.3405</td>\n",
              "      <td>3246329</td>\n",
              "      <td>49.6805</td>\n",
              "      <td>50.4605</td>\n",
              "      <td>49.5905</td>\n",
              "      <td>DUK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201876</th>\n",
              "      <td>04/30/2010</td>\n",
              "      <td>26.06</td>\n",
              "      <td>33740980</td>\n",
              "      <td>26.21</td>\n",
              "      <td>26.5</td>\n",
              "      <td>26.05</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182074</th>\n",
              "      <td>05/05/2015</td>\n",
              "      <td>82.81</td>\n",
              "      <td>2903732</td>\n",
              "      <td>83.19</td>\n",
              "      <td>83.44</td>\n",
              "      <td>82.56</td>\n",
              "      <td>PM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>05/07/2014</td>\n",
              "      <td>84.6185</td>\n",
              "      <td>70532231</td>\n",
              "      <td>85.0357</td>\n",
              "      <td>85.3271</td>\n",
              "      <td>83.9614</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156949</th>\n",
              "      <td>05/09/2016</td>\n",
              "      <td>50.07</td>\n",
              "      <td>17883360</td>\n",
              "      <td>50.49</td>\n",
              "      <td>50.585</td>\n",
              "      <td>50</td>\n",
              "      <td>MSFT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122367</th>\n",
              "      <td>05/31/2016</td>\n",
              "      <td>18.08</td>\n",
              "      <td>16117520</td>\n",
              "      <td>17.86</td>\n",
              "      <td>18.25</td>\n",
              "      <td>17.85</td>\n",
              "      <td>KMI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101334</th>\n",
              "      <td>06/07/2016</td>\n",
              "      <td>731.09</td>\n",
              "      <td>1214733</td>\n",
              "      <td>733.27</td>\n",
              "      <td>736.71</td>\n",
              "      <td>730.8</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175836</th>\n",
              "      <td>06/24/2010</td>\n",
              "      <td>62.4</td>\n",
              "      <td>6184699</td>\n",
              "      <td>62.37</td>\n",
              "      <td>62.99</td>\n",
              "      <td>62.25</td>\n",
              "      <td>PEP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105789</th>\n",
              "      <td>06/28/2018</td>\n",
              "      <td>195.21</td>\n",
              "      <td>2993911</td>\n",
              "      <td>195</td>\n",
              "      <td>196.01</td>\n",
              "      <td>193.51</td>\n",
              "      <td>HD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174572</th>\n",
              "      <td>07/02/2015</td>\n",
              "      <td>94.66</td>\n",
              "      <td>3209616</td>\n",
              "      <td>94.6</td>\n",
              "      <td>94.98</td>\n",
              "      <td>94.21</td>\n",
              "      <td>PEP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188139</th>\n",
              "      <td>07/02/2015</td>\n",
              "      <td>95.57</td>\n",
              "      <td>2163391</td>\n",
              "      <td>96.55</td>\n",
              "      <td>97.23</td>\n",
              "      <td>95.51</td>\n",
              "      <td>RTN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6149</th>\n",
              "      <td>07/03/2012</td>\n",
              "      <td>32.415</td>\n",
              "      <td>3199586</td>\n",
              "      <td>32.235</td>\n",
              "      <td>32.5</td>\n",
              "      <td>32.195</td>\n",
              "      <td>ABT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61364</th>\n",
              "      <td>07/08/2010</td>\n",
              "      <td>40.244</td>\n",
              "      <td>8716726</td>\n",
              "      <td>40.7202</td>\n",
              "      <td>40.728</td>\n",
              "      <td>39.7834</td>\n",
              "      <td>COP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11833</th>\n",
              "      <td>07/29/2019</td>\n",
              "      <td>160.97</td>\n",
              "      <td>4788688</td>\n",
              "      <td>161.16</td>\n",
              "      <td>162.14</td>\n",
              "      <td>160.65</td>\n",
              "      <td>AGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48896</th>\n",
              "      <td>08/24/2010</td>\n",
              "      <td>65.04</td>\n",
              "      <td>9620931</td>\n",
              "      <td>65.59</td>\n",
              "      <td>65.84</td>\n",
              "      <td>64.31</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201527</th>\n",
              "      <td>09/16/2011</td>\n",
              "      <td>28.94</td>\n",
              "      <td>34249290</td>\n",
              "      <td>28.82</td>\n",
              "      <td>28.94</td>\n",
              "      <td>28.54</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137961</th>\n",
              "      <td>09/26/2012</td>\n",
              "      <td>93.2</td>\n",
              "      <td>4391747</td>\n",
              "      <td>92.89</td>\n",
              "      <td>93.35</td>\n",
              "      <td>92.88</td>\n",
              "      <td>MCD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3556</th>\n",
              "      <td>10/01/2015</td>\n",
              "      <td>55.13</td>\n",
              "      <td>9247009</td>\n",
              "      <td>54.68</td>\n",
              "      <td>55.23</td>\n",
              "      <td>53.93</td>\n",
              "      <td>ABBV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110942</th>\n",
              "      <td>10/11/2017</td>\n",
              "      <td>147.62</td>\n",
              "      <td>3701258</td>\n",
              "      <td>148.4</td>\n",
              "      <td>148.47</td>\n",
              "      <td>147.28</td>\n",
              "      <td>IBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179223</th>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>88.57</td>\n",
              "      <td>17093450</td>\n",
              "      <td>88.74</td>\n",
              "      <td>89.01</td>\n",
              "      <td>88.33</td>\n",
              "      <td>PG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171508</th>\n",
              "      <td>10/16/2017</td>\n",
              "      <td>64.77</td>\n",
              "      <td>2667679</td>\n",
              "      <td>65.03</td>\n",
              "      <td>65.12</td>\n",
              "      <td>64.54</td>\n",
              "      <td>OXY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131458</th>\n",
              "      <td>10/24/2018</td>\n",
              "      <td>96.53</td>\n",
              "      <td>5723803</td>\n",
              "      <td>98.32</td>\n",
              "      <td>100.71</td>\n",
              "      <td>96.32</td>\n",
              "      <td>LOW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222866</th>\n",
              "      <td>11/10/2015</td>\n",
              "      <td>83.53</td>\n",
              "      <td>4302471</td>\n",
              "      <td>83.83</td>\n",
              "      <td>84.79</td>\n",
              "      <td>83.26</td>\n",
              "      <td>WBA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18492</th>\n",
              "      <td>11/14/2012</td>\n",
              "      <td>38.34</td>\n",
              "      <td>4824861</td>\n",
              "      <td>38.26</td>\n",
              "      <td>39.59</td>\n",
              "      <td>38.12</td>\n",
              "      <td>ALL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207686</th>\n",
              "      <td>11/17/2016</td>\n",
              "      <td>72.08</td>\n",
              "      <td>3763235</td>\n",
              "      <td>71.17</td>\n",
              "      <td>72.28</td>\n",
              "      <td>70.63</td>\n",
              "      <td>TXN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98790</th>\n",
              "      <td>11/18/2010</td>\n",
              "      <td>18.975</td>\n",
              "      <td>15700010</td>\n",
              "      <td>19.01</td>\n",
              "      <td>19.2</td>\n",
              "      <td>18.87</td>\n",
              "      <td>GILD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1285</th>\n",
              "      <td>11/20/2014</td>\n",
              "      <td>116.31</td>\n",
              "      <td>43148650</td>\n",
              "      <td>114.91</td>\n",
              "      <td>116.86</td>\n",
              "      <td>114.85</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32900</th>\n",
              "      <td>12/01/2014</td>\n",
              "      <td>288.8073</td>\n",
              "      <td>1146082</td>\n",
              "      <td>286.9253</td>\n",
              "      <td>290.7124</td>\n",
              "      <td>286.6256</td>\n",
              "      <td>BIIB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34619</th>\n",
              "      <td>12/18/2017</td>\n",
              "      <td>54.42</td>\n",
              "      <td>3975107</td>\n",
              "      <td>54.67</td>\n",
              "      <td>55</td>\n",
              "      <td>54.36</td>\n",
              "      <td>BK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151564</th>\n",
              "      <td>12/22/2017</td>\n",
              "      <td>56.36</td>\n",
              "      <td>11345730</td>\n",
              "      <td>56.6</td>\n",
              "      <td>56.83</td>\n",
              "      <td>56.13</td>\n",
              "      <td>MRK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43582</th>\n",
              "      <td>12/29/2011</td>\n",
              "      <td>35.27</td>\n",
              "      <td>6422866</td>\n",
              "      <td>35.03</td>\n",
              "      <td>35.3</td>\n",
              "      <td>34.95</td>\n",
              "      <td>BMY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Date      Close     Volume  ...       High        Low Company\n",
              "159523  01/04/2016     103.57    1929089  ...     103.65     102.34     NEE\n",
              "173214  01/05/2011    94.1728    3597597  ...    94.7261    92.5616     OXY\n",
              "15212   01/11/2016      57.69    9615965  ...         58      56.84     AIG\n",
              "95258   01/15/2015    23.5037   31431330  ...    23.8925    23.4838      GE\n",
              "219831  01/16/2018     133.97    5170230  ...     137.73     133.87     UTX\n",
              "38340   01/25/2013     718.82    1570859  ...     719.37     681.05    BKNG\n",
              "183867  02/05/2018     74.705   18709640  ...      78.98      74.68    PYPL\n",
              "211127  02/06/2013      57.38    5266009  ...      57.54       57.1     UNH\n",
              "214108  02/17/2011      48.91    6399682  ...      48.98     48.165     UNP\n",
              "166448  02/20/2018     249.08   17117670  ...     251.87      244.6    NVDA\n",
              "124920  02/26/2015      42.46   14344290  ...      42.55      41.97      KO\n",
              "9693    03/12/2018     220.94    3172266  ...     223.11     220.34    ADBE\n",
              "3690    03/23/2015      60.48    7918489  ...      61.19      59.62    ABBV\n",
              "68129   04/15/2013      56.31    6086714  ...      57.53       56.3     CVS\n",
              "16396   04/27/2011       31.7    5420339  ...      32.01      31.28     AIG\n",
              "79605   04/30/2010    50.3405    3246329  ...    50.4605    49.5905     DUK\n",
              "201876  04/30/2010      26.06   33740980  ...       26.5      26.05       T\n",
              "182074  05/05/2015      82.81    2903732  ...      83.44      82.56      PM\n",
              "1423    05/07/2014    84.6185   70532231  ...    85.3271    83.9614    AAPL\n",
              "156949  05/09/2016      50.07   17883360  ...     50.585         50    MSFT\n",
              "122367  05/31/2016      18.08   16117520  ...      18.25      17.85     KMI\n",
              "101334  06/07/2016     731.09    1214733  ...     736.71      730.8   GOOGL\n",
              "175836  06/24/2010       62.4    6184699  ...      62.99      62.25     PEP\n",
              "105789  06/28/2018     195.21    2993911  ...     196.01     193.51      HD\n",
              "174572  07/02/2015      94.66    3209616  ...      94.98      94.21     PEP\n",
              "188139  07/02/2015      95.57    2163391  ...      97.23      95.51     RTN\n",
              "6149    07/03/2012     32.415    3199586  ...       32.5     32.195     ABT\n",
              "61364   07/08/2010     40.244    8716726  ...     40.728    39.7834     COP\n",
              "11833   07/29/2019     160.97    4788688  ...     162.14     160.65     AGN\n",
              "48896   08/24/2010      65.04    9620931  ...      65.84      64.31     CAT\n",
              "201527  09/16/2011      28.94   34249290  ...      28.94      28.54       T\n",
              "137961  09/26/2012       93.2    4391747  ...      93.35      92.88     MCD\n",
              "3556    10/01/2015      55.13    9247009  ...      55.23      53.93    ABBV\n",
              "110942  10/11/2017     147.62    3701258  ...     148.47     147.28     IBM\n",
              "179223  10/12/2016      88.57   17093450  ...      89.01      88.33      PG\n",
              "171508  10/16/2017      64.77    2667679  ...      65.12      64.54     OXY\n",
              "131458  10/24/2018      96.53    5723803  ...     100.71      96.32     LOW\n",
              "222866  11/10/2015      83.53    4302471  ...      84.79      83.26     WBA\n",
              "18492   11/14/2012      38.34    4824861  ...      39.59      38.12     ALL\n",
              "207686  11/17/2016      72.08    3763235  ...      72.28      70.63     TXN\n",
              "98790   11/18/2010     18.975   15700010  ...       19.2      18.87    GILD\n",
              "1285    11/20/2014     116.31   43148650  ...     116.86     114.85    AAPL\n",
              "32900   12/01/2014   288.8073    1146082  ...   290.7124   286.6256    BIIB\n",
              "34619   12/18/2017      54.42    3975107  ...         55      54.36      BK\n",
              "151564  12/22/2017      56.36   11345730  ...      56.83      56.13     MRK\n",
              "43582   12/29/2011      35.27    6422866  ...       35.3      34.95     BMY\n",
              "\n",
              "[46 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prrbVSjrwiij",
        "colab_type": "code",
        "outputId": "e44704ce-f9d0-4abc-f5ab-3be542e4684e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "second_split.shape[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg55-oRdcugD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare Data\n",
        "def get_reg_train_test(timeseries,sequence_length):\n",
        "    # # smoothen out series\n",
        "    # if roll_mean_window:\n",
        "    #     timeseries = timeseries.rolling(roll_mean_window).mean().dropna()\n",
        "\n",
        "    # create windows\n",
        "    result = []\n",
        "    for index in range(len(timeseries) - sequence_length):\n",
        "        result.append(timeseries[index: index + sequence_length])\n",
        "\n",
        "    # normalize data as a variation of 0th index\n",
        "    # if normalize:\n",
        "    #     normalised_data = []\n",
        "    #     for window in result:\n",
        "    #         normalised_window = [((float(p) / float(window[0])) - 1) \\\n",
        "    #                                for p in window]\n",
        "    #         normalised_data.append(normalised_window)\n",
        "    #     result = normalised_data\n",
        "\n",
        "    # identify train-test splits\n",
        "    # convert to arry\n",
        "    result = np.array(result)\n",
        "    #round up 90% of rows\n",
        "    train_size = 0.9\n",
        "    row = round(train_size * result.shape[0])\n",
        "\n",
        "    # split train and test sets 90-10\n",
        "    train = result[:int(row), :]\n",
        "    test = result[int(row):, :]\n",
        "\n",
        "    # scale data in 0-1 range\n",
        "    # scaler = None\n",
        "    # if scale:\n",
        "    #     scaler=MinMaxScaler(feature_range=(0, 1))\n",
        "    #     train = scaler.fit_transform(train)\n",
        "    #     test = scaler.transform(test)\n",
        "\n",
        "    # split independent and dependent variables\n",
        "    x_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "\n",
        "\n",
        "    x_test = test[:, :-1]\n",
        "    y_test = test[:, -1]\n",
        "\n",
        "    # Transforms for LSTM input\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0],\n",
        "                                   x_train.shape[1],\n",
        "                                   1))\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0],\n",
        "                                 x_test.shape[1],\n",
        "                                 1))\n",
        "\n",
        "    return x_train,y_train,x_test,y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYDjMwcxdk0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_test,y_test = get_reg_train_test(second_split.Close, sequence_length=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Swk7vPcpU4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(y_train)\n",
        "print(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os7wXJaiIy7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6bRYECCxii2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_reg_model(layer_units=[100,100],dropouts=[0.2,0.2],window_size=50):\n",
        "    # build LSTM network\n",
        "    model = Sequential()\n",
        "\n",
        "    # hidden layer 1\n",
        "    model.add(LSTM(layer_units[0],\n",
        "                   input_shape=(window_size,1),\n",
        "                   return_sequences=True))\n",
        "    model.add(Dropout(dropouts[0]))\n",
        "\n",
        "    # hidden layer 2\n",
        "    model.add(LSTM(layer_units[1]))\n",
        "    model.add(Dropout(dropouts[1]))\n",
        "\n",
        "    # output layer\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation(\"linear\"))\n",
        "\n",
        "    start = time.time()\n",
        "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
        "    print(\"> Compilation Time : \", time.time() - start)\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89FoYAFnFly0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Window wise prediction function\n",
        "def predict_reg_multiple(model, data, window_size=6, prediction_len=3):\n",
        "    prediction_list = []\n",
        "    \n",
        "    # loop for every sequence in the dataset\n",
        "    for window in range(int(len(data)/prediction_len)):\n",
        "        _seq = data[window*prediction_len]\n",
        "        predicted = []\n",
        "        # loop till required prediction length is achieved\n",
        "        for j in range(prediction_len):\n",
        "            predicted.append(model.predict(_seq[np.newaxis,:,:])[0,0])\n",
        "            _seq = _seq[1:]\n",
        "            _seq = np.insert(_seq, [window_size-1], predicted[-1], axis=0)\n",
        "        prediction_list.append(predicted)\n",
        "    return prediction_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47sgQQYuJKrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot window wise \n",
        "def plot_reg_results(predicted_data, true_data, prediction_len=3):\n",
        "    fig = plt.figure(facecolor='white')\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    # plot actual data\n",
        "    ax.plot(true_data, \n",
        "            label='True Data',\n",
        "            c='black',alpha=0.3)\n",
        "    \n",
        "    # plot flattened data\n",
        "    plt.plot(np.array(predicted_data).flatten(), \n",
        "             label='Prediction_full',\n",
        "             c='g',linestyle='--')\n",
        "    \n",
        "    #plot each window in the prediction list\n",
        "    for i, data in enumerate(predicted_data):\n",
        "        padding = [None for p in range(i * prediction_len)]\n",
        "        plt.plot(padding + data, label='Prediction',c='black')\n",
        "\n",
        "    plt.title(\"Forecast Plot with Prediction Window={}\".format(prediction_len))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSjdrXlnXJTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set Parameters\n",
        "WINDOW=6\n",
        "PRED_LENGTH = int(WINDOW/2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ctLk1glxiFm",
        "colab_type": "code",
        "outputId": "ac9fdf5c-31af-4dca-c96e-b33c168bf121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        " lstm_model=None\n",
        " try:\n",
        "   lstm_model = get_reg_model(layer_units=[50,100],\n",
        "                               window_size=WINDOW)\n",
        " except:\n",
        "   print(\"Model Build Failed. Trying Again\")\n",
        "   lstm_model = get_reg_model(layer_units=[50,100],\n",
        "                               window_size=WINDOW)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Compilation Time :  0.01779031753540039\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (None, 6, 50)             10400     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 6, 50)             0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 101       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 70,901\n",
            "Trainable params: 70,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gnuEFkC7fuy",
        "colab_type": "code",
        "outputId": "081b823b-d667-430e-d95f-f267fc67c885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "   # use eatrly stopping to avoid overfitting\n",
        "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                               patience=2,\n",
        "                                               verbose=0)]\n",
        "    lstm_model.fit(x_train, y_train,\n",
        "                   epochs=20, batch_size=16,\n",
        "                   verbose=1,validation_split=0.05,\n",
        "                   callbacks=callbacks)\n",
        "    print(\"Model Fit Complete\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33 samples, validate on 2 samples\n",
            "Epoch 1/20\n",
            "33/33 [==============================] - 1s 45ms/step - loss: 25467.2330 - val_loss: 2549.3665\n",
            "Epoch 2/20\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24957.0319 - val_loss: 2354.3452\n",
            "Epoch 3/20\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24524.2678 - val_loss: 2115.6616\n",
            "Epoch 4/20\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23900.1326 - val_loss: 1958.7139\n",
            "Epoch 5/20\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23569.5023 - val_loss: 1819.8834\n",
            "Epoch 6/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 23206.7351 - val_loss: 1729.9126\n",
            "Epoch 7/20\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22923.7236 - val_loss: 1701.4447\n",
            "Epoch 8/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22912.3554 - val_loss: 1674.0914\n",
            "Epoch 9/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22896.4076 - val_loss: 1647.4503\n",
            "Epoch 10/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22790.6261 - val_loss: 1620.3577\n",
            "Epoch 11/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22741.5033 - val_loss: 1605.1495\n",
            "Epoch 12/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22636.1840 - val_loss: 1589.1538\n",
            "Epoch 13/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22643.3505 - val_loss: 1576.4980\n",
            "Epoch 14/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22622.8464 - val_loss: 1563.8491\n",
            "Epoch 15/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22541.0379 - val_loss: 1549.5712\n",
            "Epoch 16/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22507.9032 - val_loss: 1536.2504\n",
            "Epoch 17/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22481.0564 - val_loss: 1523.4631\n",
            "Epoch 18/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22439.4658 - val_loss: 1507.1796\n",
            "Epoch 19/20\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22394.1782 - val_loss: 1491.6364\n",
            "Epoch 20/20\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22329.7481 - val_loss: 1477.8431\n",
            "Model Fit Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI4qNY1H96vL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8a65b82c-e5a6-4428-f1e9-00b7254a5311"
      },
      "source": [
        "#Train Prediction Performance\n",
        "train_pred_seqs = predict_reg_multiple(lstm_model,\n",
        "                                             x_train,\n",
        "                                             window_size=WINDOW,\n",
        "                                             prediction_len=PRED_LENGTH)\n",
        "\n",
        "train_offset = y_train.shape[0] - np.array(train_pred_seqs).flatten().shape[0]\n",
        "\n",
        "train_rmse = math.sqrt(mean_squared_error(y_train[train_offset:], \n",
        "                                          np.array(train_pred_seqs).\\\n",
        "                                          flatten()))\n",
        "print('Train Score: %.2f RMSE' % (train_rmse))\n",
        "print(train_pred_seqs)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 149.13 RMSE\n",
            "[[17.72832, 17.72879, 17.728704], [17.726831, 17.727465, 17.728191], [17.72861, 17.72826, 17.727663], [17.728006, 17.72775, 17.728558], [17.729076, 17.729261, 17.728893], [17.728949, 17.728079, 17.728796], [17.728844, 17.72822, 17.72555], [17.72758, 17.727812, 17.72804], [17.728579, 17.728992, 17.72826], [17.727932, 17.72867, 17.729097], [17.727974, 17.72822, 17.72795]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isxRUMwPGGPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4316e7db-143e-4058-e9d0-39da7258b4bf"
      },
      "source": [
        "#Test Prediction Performance\n",
        "test_pred_seqs = predict_reg_multiple(lstm_model,\n",
        "                                      x_test,\n",
        "                                      window_size=WINDOW,\n",
        "                                      prediction_len=PRED_LENGTH)\n",
        "test_pred_seqs\n",
        "test_offset = y_test.shape[0] - np.array(test_pred_seqs).flatten().shape[0]\n",
        "\n",
        "test_rmse = math.sqrt(mean_squared_error(y_test[test_offset:], \n",
        "                                          np.array(test_pred_seqs).\\\n",
        "                                          flatten()))\n",
        "print('Test Score: %.2f RMSE' % (test_rmse))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score: 159.50 RMSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHdSg7JTJD7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "953d594d-5d13-4750-b089-82cfd5441866"
      },
      "source": [
        "plot_reg_results(test_pred_seqs,y_test,prediction_len=PRED_LENGTH)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEICAYAAAB8lNKlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB2eRlhp8eZuyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voelLbAe822y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n"
      ]
    }
  ]
}